Numerical data may once have been irrelevant in the halls of academia and the bustling classrooms full of students, but in an era that increasingly emphasizes written *standards* over tradition, this is no longer desirable. (Carpinelli 2008) If standards were designed to create equitable student outcomes across different schools or areas, how are educators to know whether their goal has been met? 

Evaluation of standards-based programs may be desirable but impossible due to lack of data availability. Although great effort has been spent on preparing standards-based curriculum and standards-based testing strategies, Carpinelli questions whether this effort has translated into lesson plans that teachers generate on a daily basis. Thus, the aim of this project is to examine these ground-level documents that may represent the closest data to the act of teaching itself. (Carpinelli 2008)

But why is evaluation necessary? In previous studies, two primary reasons for evaluation occur: to identify teacher efficacy in educating students, and to identify learning outcomes of students themselves. Torff puts plainly why it is necessary to evaluate teacher efficacy: the increasing demand for teachers is not being supplied by traditional avenues, so a variety of alternative pipelines for producing certified teachers have emerged. Whether these pipelines are producing effective teachers is unclear. (Torff 2009)

There are two main approaches to assesing teacher effectiveness: quantitative and qualitative. The quantitative approach has generally meant reducing teachers to a collection of student test grades. The qualitative approach gives principals and other supervisors the task of assesing teacher competence, but potential bias reigns. Torff leaves undiscussed the idea of quantitively assesing teacher's on the basis of their actual output rather than student success. (Torff 2009)

At least as important as evaluating teachers is evalutating student outcomes, but in some instances the desired accountability is merely theoretical. Avery laments the simple "absence of data on student learning" in the context of gifted learning programs. While it is suggested that states may simply be focusing on the larger body of students, if states cannot make data-driven decisions in the context of a relatively small program, how can they hope to do so as scale dramatically increases? And this is not a forgotten issue: in interviews with various stakeholders, a consistent complaint was the lack of available data. (Avery 2001)

One of the few attempts to look at bottom-up data is Carpinelli. In their study, they developed a rubric-based grading model for lesson plans and recruited and trained a panel to evaluate 200 lesson plans from a variety of sources. They identified several common problems across lesson plans: the use of too many or incorrect standards, lesson objectives that simply do not correlate to stated standards, and assesments that do not correlate it either standards or objectives. Unfortunately, the inter-rater agreement on which lesson plans were poor was weak. (Carpinelli 2008)

What are schools to make of the lack of data, the lack of good ways to interpret it, or the lack of willingness to use it? As it turns out, how to use data to drive decision making has long been a focus of the corporate world, where it is often discussed as "Business Intelligence."

